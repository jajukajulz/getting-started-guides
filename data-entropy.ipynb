{
 "cells": [
  {
   "attachments": {
    "256px-Entropy-mutual-information-relative-entropy-relation-diagram.svg.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 ENTROPY\n",
    "\n",
    "- Measure of uncertainty of a random variable $X$. The higher the entropy, the more the uncertainty i.e. more privacy and vice-versa.\n",
    "- Discrete Entropy applies to Discrete Random Variables whilst Differential Entropy applies to  Continuous Random Variables.\n",
    "\n",
    "\n",
    "![256px-Entropy-mutual-information-relative-entropy-relation-diagram.svg.png](attachment:256px-Entropy-mutual-information-relative-entropy-relation-diagram.svg.png)\n",
    "Venn diagram showing additive and subtractive relationships of various information measures associated with correlated variables {\\displaystyle X}X and {\\displaystyle Y}Y. The area contained by both circles is the joint entropy {\\displaystyle \\mathrm {H} (X,Y)}{\\displaystyle \\mathrm {H} (X,Y)}. The circle on the left (red and violet) is the individual entropy {\\displaystyle \\mathrm {H} (X)}{\\displaystyle \\mathrm {H} (X)}, with the red being the conditional entropy {\\displaystyle \\mathrm {H} (X\\mid Y)}{\\displaystyle \\mathrm {H} (X\\mid Y)}. The circle on the right (blue and violet) is {\\displaystyle \\mathrm {H} (Y)}{\\displaystyle \\mathrm {H} (Y)}, with the blue being {\\displaystyle \\mathrm {H} (Y\\mid X)}{\\displaystyle \\mathrm {H} (Y\\mid X)}. The violet is the mutual information {\\displaystyle \\operatorname {I} (X;Y)}\\operatorname {I} (X;Y).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Discrete Entropy (Discrete Random Variables)\n",
    "- Let $X$ be a discrete random variable with alphabet $X$ and probability mass function $p(x) = Pr({X = x})$, $x \\epsilon X$.\n",
    "- The entropy $H(X)$ of a discrete random variable $X$ is defined by\n",
    "$$H(X) = -\\sum_{x \\epsilon X}^n p(x) log p(x)$$\n",
    "\n",
    "- NB: The log is to the base 2 and entropy is expressed in bits.\n",
    "- $H(X) \\geqslant 0$\n",
    "- Conditioning reduces entropy (see section on conditional entropy) i.e. $H(X|Y) \\leqslant H(X)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of fair coin toss is 1.0 bit\n",
      "Manual check of Entropy of fair coin toss is 1.0 bit\n"
     ]
    }
   ],
   "source": [
    "# For example, we will show that the entropy of a fair coin toss is 1 bit.\n",
    "'''\n",
    "Let X be the outcome of a coin toss - {H,T}\n",
    "P(X) = 0.5 for each outcome since it is a fair coin\n",
    "\n",
    "----|----|----|\n",
    "X   |  H | T  |\n",
    "----|----|----|\n",
    "P(x)| 0.5|0.5 |\n",
    "----|----|----|\n",
    "\n",
    "'''\n",
    "import math\n",
    "\n",
    "#calculating discrete entropy from first principles (alternative would be to use\n",
    "#scipy library\n",
    "LOG_BASE_2 = 2\n",
    "OUTCOMES = ['H', 'T']\n",
    "DISTRIBUTION = [0.5, 0.5]\n",
    "entropy_H_X = -( sum( (i * math.log(i, LOG_BASE_2)) for i in DISTRIBUTION ) )\n",
    "print('Entropy of fair coin toss is %s bit' % entropy_H_X)\n",
    "\n",
    "#manual calculation\n",
    "entropy_H_X_test = -( (0.5 * math.log(0.5, 2)) + (0.5 * math.log(0.5, 2)) )\n",
    "print('Manual check of Entropy of fair coin toss is %s bit' % entropy_H_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Conditional Entropy and Mutual Information  (Discrete Random Variables)\n",
    "- Conditional entropy is the entropy of a random variable that is conditional on the knowledge of another.\n",
    "- Conditioning reduces entropy i.e. $H(X|Y) \\leqslant H(X)$\n",
    "- Reduction in uncertainity due to another random variable is called Mutual Information(which is denoted as $I$)\n",
    "- For 2 random variables, $X$ & $Y$:\n",
    "$$ I(X;Y) = H(X) - H(X|Y) = \\sum_{x, y}^n p(x,y) log \\frac{p(x,y)}{p(x)p(y)} $$\n",
    "\n",
    "-Mutual information $I(X;Y)$ is a measure of dependence between 2 random variables, is symmetric, always non-negative and is zero if $X$ & $Y$ are independent. In the case of privacy applications, we want mutual information to be as low or close to zero as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy libary calculation of Entropy for a fair coin toss is 1.0 bit\n",
      "scipy libary calculation of Entropy for a biased coin toss is 0.46899559358928117 bit\n",
      "scipy libary calculation of relative entropy of a biased coin toss is 0.5108256237659907 bit\n"
     ]
    }
   ],
   "source": [
    "'''Using scipy to calculate the entropy of a fair coin toss which is 1 bit.\n",
    "i.e. a Bernoulli trial with same p (0.5 Heads, 0.5 Tails). The outcome of a fair coin is the most uncertain unlike \n",
    "one in which the coin is not fair p (0.9 Heads, 0.1 Tails)\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html#:~:text=Calculate%20the%20entropy%20of%20a,)%2C%20axis%3Daxis)%20.\n",
    "Calculate the entropy of a distribution for given probability values.\n",
    "\n",
    "If only probabilities pk are given, the entropy is calculated as S = -sum(pk * log(pk), axis=axis).\n",
    "\n",
    "If qk is not None, then compute the Kullback-Leibler divergence S = sum(pk * log(pk / qk), axis=axis)\n",
    "Parameters\n",
    "pk: sequence of prior probabilities\n",
    "Defines the (discrete) distribution. pk[i] is the (possibly unnormalized) probability of event i.\n",
    "\n",
    "qk: sequence of posterior probabilities, optional\n",
    "Sequence against which the relative entropy is computed. Should be in the same format as pk.\n",
    "\n",
    "basefloat, optional\n",
    "The logarithmic base to use, defaults to e (natural logarithm).\n",
    "\n",
    "axis: int, optional\n",
    "The axis along which the entropy is calculated. Default is 0.\n",
    "\n",
    "Returns\n",
    "Sfloat\n",
    "The calculated entropy.'''\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "#The outcome of a fair coin is very uncertain:\n",
    "scipy_discrete_fair_entropy = entropy([1/2, 1/2], base=2)\n",
    "print('scipy libary calculation of Entropy for a fair coin toss is %s bit' % scipy_discrete_fair_entropy)\n",
    "\n",
    "#The outcome of a biased coin is less uncertain:\n",
    "scipy_discrete_biased_entropy = entropy([9/10, 1/10], base=2)\n",
    "print('scipy libary calculation of Entropy for a biased coin toss is %s bit' % scipy_discrete_biased_entropy)\n",
    "\n",
    "#Relative entropy\n",
    "scipy_relative_biased_entropy = entropy([1/2, 1/2], qk=[9/10, 1/10])\n",
    "print('scipy libary calculation of relative entropy of a biased coin toss is %s bit' % scipy_relative_biased_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Differential Entropy (Continuous Random Variables)\n",
    "- Differential entropy differs from normal or absolute entropy in that the random variable need not be discrete.\n",
    "- Interpretation of $h(X)$ is the measure for amount of information we do not have about X\n",
    "- Let X be a continuous random variable with cumulative distribution function $F(x) = Pr(X \\leqslant x)$ and probability density function $f(x)$ i.e. the PDF for $X$ is the first derivative of $F(x)$ which can be written as $F^\\prime(x)$.\n",
    "- The differential entropy $h(X)$ of a continuous random variable $X$ with density $f(x)$ is defined as \n",
    "$$h(X) = -\\int_S f(x) log f(x) \\delta x$$\n",
    "where $S$ is the support set of the random variable (i.e. where $f(x) > 0)$\n",
    "\n",
    "- Example (Uniform distribution) - Consider a continuous random variable, $X$, distributed uniformly from $0$ to $a$, with $a$ > 0 and density  $f(x)$ is $1/a$ from 0 to $a$ and 0 elsewhere. Then its differential entropy of $X$ is\n",
    "$$h(X) = -\\int_0^a \\frac{1}{a} log \\frac{1}{a} \\delta x$$\n",
    "\n",
    "- Differential entropy can be negative\n"
   ]
  },
  {
   "attachments": {
    "mututal_information.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Joint Differential Entropy\n",
    "- The differential entropy of a set $X_{1}, X_{2},...X_{n}$ of continuous random variables with density $f(x_{1}, x_{2},...,x_{n})$ is\n",
    "$$h(X_{1}, X_{2},...X_{n}) = -\\int f(x^n) log f(x^n) \\delta x^n $$\n",
    "\n",
    "### 1.2.2 Conditional Differential Entropy\n",
    "- The differential entropy of a set $X, Y$ of continuous random variables with a joint density function $f(x, y)$ is\n",
    "$$h(X|Y) = -\\int f(x,y) log f(x|y) \\delta x \\delta y $$\n",
    "\n",
    "which can also be written as $h(X|Y) = h(X,Y) - h(Y)$ since generally bayes suggests that $f(x|y) = \\frac{f(x,y)}{f(y)}$\n",
    "\n",
    "### 1.2.3 Relative Entropy and Mutual Information\n",
    "- The relative entropy (Kullback-Leibler divergence) between two continuous densities $f$ and $g$ measures the difference between two probbility distributions and is given by\n",
    "$$ D(f||g) = \\int f log \\frac{f}{g}$$\n",
    "- The main interpretation of relative entropy is that the information we gained about $X$, if we originally thought $X ∼ g$ and now we learned X ∼ $p$ i.e. Expressed in the language of Bayesian inference, $D(f||g)$ is a measure of the information gained by revising one's beliefs from the prior probability distribution $g$ to the posterior probability distribution $p$.  In order to find a distribution $g$ that is closest to $f$, we can minimize KL divergence and compute an information projection.\n",
    "\n",
    "### 1.2.4 Mutual Information (Information Gain)\n",
    "Mutual information is a distance between two probability distributions. Correlation is a linear distance between two random variables. Mutual Information is also known as information gain.\n",
    "\n",
    "In probability theory and information theory, the mutual information (MI) of two random variables is a measure of the mutual dependence between the two variables. More specifically, it quantifies the \"amount of information\" (in units such as shannons (bits), nats or hartleys) obtained about one random variable through observing the other random variable. The concept of mutual information is intimately linked to that of entropy of a random variable, a fundamental notion in information theory that quantifies the expected \"amount of information\" held in a random variable.\n",
    "\n",
    "Intuitively, mutual information measures the information that {\\displaystyle X}X and {\\displaystyle Y}Y share: It measures how much knowing one of these variables reduces uncertainty about the other. For example, if {\\displaystyle X}X and {\\displaystyle Y}Y are independent, then knowing {\\displaystyle X}X does not give any information about {\\displaystyle Y}Y and vice versa, so their mutual information is zero. At the other extreme, if {\\displaystyle X}X is a deterministic function of {\\displaystyle Y}Y and {\\displaystyle Y}Y is a deterministic function of {\\displaystyle X}X then all information conveyed by {\\displaystyle X}X is shared with {\\displaystyle Y}Y: knowing {\\displaystyle X}X determines the value of {\\displaystyle Y}Y and vice versa. As a result, in this case the mutual information is the same as the uncertainty contained in {\\displaystyle Y}Y (or {\\displaystyle X}X) alone, namely the entropy of {\\displaystyle Y}Y (or {\\displaystyle X}X). Moreover, this mutual information is the same as the entropy of {\\displaystyle X}X and as the entropy of {\\displaystyle Y}Y. (A very special case of this is when {\\displaystyle X}X and {\\displaystyle Y}Y are the same random variable.)\n",
    "\n",
    "\n",
    "- Definition 1: The mutual information $I(X;Y)$ between two continuous random variables  $X$ & $Y$ is given by\n",
    "$$ I(X;Y) = h(X) - h(X|Y) = h(Y) - h(Y|X) $$\n",
    "\n",
    "\n",
    "-  Definition 2: The mutual information $I(X;Y)$ between two continuous random variables with joint density $f(x,y)$ is given by\n",
    "$$ I(X;Y) = \\int f(x,y) log \\frac{f(x,y)}{f(x)f(y)} \\delta x \\delta y = E\\left[log\\frac{f(X,Y)}{f(X)f(Y)}\\right]$$\n",
    "\n",
    "\n",
    "**Mutual information versus correlation**\n",
    "\n",
    "A measure used to represent how strongly two random variables are related known as correlation. Measures of correlation include covariance, correlation coefficient (a correlation coefficient is a quantitative assessment that measures both the direction and the strength of this tendency to vary together. Values can range from -1 to +1. -1 is an extremely strong negative relationship, 0 is no relationship, +1 is a very strong positivie relationship) etc. \n",
    "\n",
    "Mutual information is a distance between two probability distributions. Correlation is a linear distance between two random variables.\n",
    "\n",
    "In these two plots the correlation coefficient is zero - Pearson’s correlation coefficients measure only linear relationships so if your data contain a curvilinear relationship, the correlation coefficient will not detect it. For example, the correlation for the data in the scatterplot below is zero. However, there is a relationship between the two variables—it’s just not linear. But we can get high shared mutual information even when the correlation is zero - just because the coefficient is near zero, it doesn’t necessarily indicate that there is no relationship.\n",
    "\n",
    "In the first, I see that if I have a high or low value of X then I'm likely to get a high value of Y. But if the value of X is moderate then I have a low value of Y. The first plot holds information about the mutual information shared by X and Y. In the second plot, X tells me nothing about Y.\n",
    "\n",
    "![mututal_information.png](attachment:mututal_information.png)\n",
    "\n",
    "Mutual information and correlation are not antagonistic—they are complementary, describing different aspects of the association between two random variables. Loosely, Mutual Information \"is not concerned\" whether the association is linear or not, while Covariance may be zero and the variables may still be stochastically dependent. On the other hand, Covariance can be calculated directly from a data sample without the need to actually know the probability distributions involved (since it is an expression involving moments of the distribution), while Mutual Information requires knowledge of the probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Estimating differential entropy\n",
    "Estimate entropy exercies, for each, for each, \n",
    "estimate the entropy based on samples of 4 different sizes i.e. N: 10,100,1000,10000.\n",
    "1. Uniform distribution on the interval [0, 1].\n",
    "2. Uniform distribution on the interval [0, 8].\n",
    "3. Uniform distribution on the interval [0, 0.5].\n",
    "4. Gaussian distribution with mean 0 and standard deviation 1.\n",
    "5. Gaussian distribution with mean 0 and standard deviation 100.\n",
    "6. Exponential distribution with mean 1.\n",
    "7. Exponential distribution with mean 100.\n",
    "\n",
    "Parametric statistics are based on assumptions about the distribution of population from \n",
    "which the sample was taken. Nonparametric statistics are not based on assumptions, that is, \n",
    "the data can be collected from a sample that does not follow a specific distribution.\n",
    "'''\n",
    "\n",
    "SAMPLE_SIZES = [10,100,1000,10000]\n",
    "\n",
    "#list to store monte carlo simulation entropies\n",
    "entropy_monte_carlo = []\n",
    "\n",
    "\n",
    "#create distribution\n",
    "xs = []\n",
    "\n",
    "#create standard deviation for distribution \n",
    "sigma = 1\n",
    "\n",
    "def samplingEntropyEst(xs, N, sigma):\n",
    "    '''\n",
    "    Since differential entropy is an expectation (of the negative log probability density), \n",
    "    we can estimate it by sampling from a distribution and forming an empirical average of the \n",
    "    negative log probability density. This empirical average will converge to the \n",
    "    true differential entropy by the law of large numbers.\n",
    "\n",
    "    Params:\n",
    "    xs is the sample points in the distribution\n",
    "    N is the number of times to sample in the Monte Carlo procedure\n",
    "    sigma is the standard deviation of the sample - need to find a way of generating an \n",
    "    optimal sigma\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "for N in SAMPLE_SIZES:\n",
    "    entropy_monte_carlo.append(samplingEntropyEst(xs, N, sigma))\n",
    "\n",
    "def mspacingEntropyEst(xs):\n",
    "    '''\n",
    "    m−spacings estimate of entropy.\n",
    "    One way of choosing m  is to use the nearest integer value to the square root of N, the\n",
    "    number of sample points.\n",
    "\n",
    "    Params:\n",
    "    xs is the sample points in the distribution\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "entropy2 = mspacingEntropyEst(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
