{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 ENTROPY\n",
    "\n",
    "- Measure of uncertainty of a random variable $X$. The higher the entropy, the more the uncertainty i.e. more privacy and vice-versa.\n",
    "- Discrete Entropy applies to Discrete Random Variables whilst Differential Entropy applies to  Continuous Random Variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Discrete Entropy (Discrete Random Variables)\n",
    "- Let $X$ be a discrete random variable with alphabet $X$ and probability mass function $p(x) = Pr({X = x})$, $x \\epsilon X$.\n",
    "- The entropy $H(X)$ of a discrete random variable $X$ is defined by\n",
    "$$H(X) = -\\sum_{x \\epsilon X}^n p(x) log p(x)$$\n",
    "\n",
    "- NB: The log is to the base 2 and entropy is expressed in bits.\n",
    "- $H(X) \\geqslant 0$\n",
    "- Conditioning reduces entropy (see section on conditional entropy) i.e. $H(X|Y) \\leqslant H(X)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of fair coin toss is 1.0 bit\n",
      "Manual check of Entropy of fair coin toss is 1.0 bit\n"
     ]
    }
   ],
   "source": [
    "# For example, we will show that the entropy of a fair coin toss is 1 bit.\n",
    "'''\n",
    "Let X be the outcome of a coin toss - {H,T}\n",
    "P(X) = 0.5 for each outcome since it is a fair coin\n",
    "\n",
    "----|----|----|\n",
    "X   |  H | T  |\n",
    "----|----|----|\n",
    "P(x)| 0.5|0.5 |\n",
    "----|----|----|\n",
    "\n",
    "'''\n",
    "import math\n",
    "\n",
    "#calculating discrete entropy from first principles (alternative would be to use\n",
    "#scipy library\n",
    "LOG_BASE_2 = 2\n",
    "OUTCOMES = ['H', 'T']\n",
    "DISTRIBUTION = [0.5, 0.5]\n",
    "entropy_H_X = -( sum( (i * math.log(i, LOG_BASE_2)) for i in DISTRIBUTION ) )\n",
    "print('Entropy of fair coin toss is %s bit' % entropy_H_X)\n",
    "\n",
    "#manual calculation\n",
    "entropy_H_X_test = -( (0.5 * math.log(0.5, 2)) + (0.5 * math.log(0.5, 2)) )\n",
    "print('Manual check of Entropy of fair coin toss is %s bit' % entropy_H_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Conditional Entropy and Mutual Information  (Discrete Random Variables)\n",
    "- Conditional entropy is the entropy of a random variable that is conditional on the knowledge of another.\n",
    "- Conditioning reduces entropy i.e. $H(X|Y) \\leqslant H(X)$\n",
    "- Reduction in uncertainity due to another random variable is called Mutual Information(which is denoted as $I$)\n",
    "- For 2 random variables, $X$ & $Y$:\n",
    "$$ I(X;Y) = H(X) - H(X|Y) = \\sum_{x, y}^n p(x,y) log \\frac{p(x,y)}{p(x)p(y)} $$\n",
    "\n",
    "-Mutual information $I(X;Y)$ is a measure of dependence between 2 random variables, is symmetric, always non-negative and is zero if $X$ & $Y$ are independent. In the case of privacy applications, we want mutual information to be as low or close to zero as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Differential Entropy (Continuous Random Variables)\n",
    "- Differential entropy differs from normal or absolute entropy in that the random variable need not be discrete.\n",
    "- Interpretation of $h(X)$ is the measure for amount of information we do not have about X\n",
    "- Let X be a continuous random variable with cumulative distribution function $F(x) = Pr(X \\leqslant x)$ and probability density function $f(x)$ i.e. the PDF for $X$ is the first derivative of $F(x)$ which can be written as $F^\\prime(x)$.\n",
    "- The differential entropy $h(X)$ of a continuous random variable $X$ with density $f(x)$ is defined as \n",
    "$$h(X) = -\\int_S f(x) log f(x) \\delta x$$\n",
    "where $S$ is the support set of the random variable (i.e. where $f(x) > 0)$\n",
    "\n",
    "- Example (Uniform distribution) - Consider a continuous random variable, $X$, distributed uniformly from $0$ to $a$, with $a$ > 0 and density  $f(x)$ is $1/a$ from 0 to $a$ and 0 elsewhere. Then its differential entropy of $X$ is\n",
    "$$h(X) = -\\int_0^a \\frac{1}{a} log \\frac{1}{a} \\delta x$$\n",
    "\n",
    "- Differential entropy can be negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Joint Differential Entropy\n",
    "- The differential entropy of a set $X_{1}, X_{2},...X_{n}$ of continuous random variables with density $f(x_{1}, x_{2},...,x_{n})$ is\n",
    "$$h(X_{1}, X_{2},...X_{n}) = -\\int f(x^n) log f(x^n) \\delta x^n $$\n",
    "\n",
    "### 1.2.2 Conditional Differential Entropy\n",
    "- The differential entropy of a set $X, Y$ of continuous random variables with a joint density function $f(x, y)$ is\n",
    "$$h(X|Y) = -\\int f(x,y) log f(x|y) \\delta x \\delta y $$\n",
    "\n",
    "which can also be written as $h(X|Y) = h(X,Y) - h(Y)$ since generally bayes suggests that $f(x|y) = \\frac{f(x,y)}{f(y)}$\n",
    "\n",
    "### 1.2.3 Relative Entropy and Mutual Information\n",
    "- The relative entropy (Kullback-Leibler divergence) between two densities $f$ and $g$ measures the difference between two probbility distributions and is given by\n",
    "$$ D(f||g) = \\int f log \\frac{f}{g}$$\n",
    "- The main interpretation of relative entropy is that the information we gained about $X$, if we originally thought $X ∼ g$ and now we learned X ∼ $p$ i.e. Expressed in the language of Bayesian inference, $D(f||g)$ is a measure of the information gained by revising one's beliefs from the prior probability distribution $g$ to the posterior probability distribution $p$.  In order to find a distribution $g$ that is closest to $f$, we can minimize KL divergence and compute an information projection.\n",
    "\n",
    "- The mutual information $I(X;Y)$ between two continuous random variables with join density $f(x,y)$ is given by\n",
    "$$ I(X;Y) = \\int f(x,y) log \\frac{f(x,y)}{f(x)f(y)} \\delta x \\delta y $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Estimating differential entropy\n",
    "Estimate entropy exercies, for each, for each, \n",
    "estimate the entropy based on samples of 4 different sizes i.e. N: 10,100,1000,10000.\n",
    "1. Uniform distribution on the interval [0, 1].\n",
    "2. Uniform distribution on the interval [0, 8].\n",
    "3. Uniform distribution on the interval [0, 0.5].\n",
    "4. Gaussian distribution with mean 0 and standard deviation 1.\n",
    "5. Gaussian distribution with mean 0 and standard deviation 100.\n",
    "6. Exponential distribution with mean 1.\n",
    "7. Exponential distribution with mean 100.\n",
    "\n",
    "Parametric statistics are based on assumptions about the distribution of population from \n",
    "which the sample was taken. Nonparametric statistics are not based on assumptions, that is, \n",
    "the data can be collected from a sample that does not follow a specific distribution.\n",
    "'''\n",
    "\n",
    "SAMPLE_SIZES = [10,100,1000,10000]\n",
    "\n",
    "#list to store monte carlo simulation entropies\n",
    "entropy_monte_carlo = []\n",
    "\n",
    "\n",
    "#create distribution\n",
    "xs = []\n",
    "\n",
    "#create standard deviation for distribution \n",
    "sigma = 1\n",
    "\n",
    "def samplingEntropyEst(xs, N, sigma):\n",
    "    '''\n",
    "    Since differential entropy is an expectation (of the negative log probability density), \n",
    "    we can estimate it by sampling from a distribution and forming an empirical average of the \n",
    "    negative log probability density. This empirical average will converge to the \n",
    "    true differential entropy by the law of large numbers.\n",
    "\n",
    "    Params:\n",
    "    xs is the sample points in the distribution\n",
    "    N is the number of times to sample in the Monte Carlo procedure\n",
    "    sigma is the standard deviation of the sample - need to find a way of generating an \n",
    "    optimal sigma\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "for N in SAMPLE_SIZES:\n",
    "    entropy_monte_carlo.append(samplingEntropyEst(xs, N, sigma))\n",
    "\n",
    "def mspacingEntropyEst(xs):\n",
    "    '''\n",
    "    m−spacings estimate of entropy.\n",
    "    One way of choosing m  is to use the nearest integer value to the square root of N, the\n",
    "    number of sample points.\n",
    "\n",
    "    Params:\n",
    "    xs is the sample points in the distribution\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "entropy2 = mspacingEntropyEst(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}