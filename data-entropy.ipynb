{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTROPY\n",
    "\n",
    "## Definition\n",
    "- Measure of uncertainty of a random variable X. The higher the entropy, the more the uncertainty i.e. more privacy and vice-versa.\n",
    "- Discrete Entropy for Discrete Random Variables and Differential Entropy for Continuous Random Variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Entropy (Discrete Random Variables)\n",
    "- Let $X$ be a discrete random variable with alphabet $X$ and probability mass function $p(x) = Pr{X = x}$, $x \\epsilon X$.\n",
    "- The entropy $H(X)$ of a discrete random variable $X$ is defined by\n",
    "$$H(X) = -\\sum_{x \\epsilon X}^n p(x) log p(x)$$\n",
    "\n",
    "- NB: The log is to the base 2 and entropy is expressed in bits.\n",
    "- H(X) > = 0\n",
    "- Conditioning reduces entropy i.e. H(X|Y) <= H(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of fair coin toss is 1.0 bit\n"
     ]
    }
   ],
   "source": [
    "# For example, we will show that the entropy of a fair coin toss is 1 bit.\n",
    "'''\n",
    "Let X be the outcome of a coin toss - {H,T}\n",
    "P(X) = 0.5 for each outcome since it is a fair coin\n",
    "\n",
    "----|----|----|\n",
    "X   |  H | T  |\n",
    "----|----|----|\n",
    "P(x)| 0.5|0.5 |\n",
    "----|----|----|\n",
    "\n",
    "'''\n",
    "import math\n",
    "\n",
    "entropy_H_X = -( (0.5 * math.log(0.5, 2)) + (0.5 * math.log(0.5, 2)) )\n",
    "print('Entropy of fair coin toss is %s bit' % entropy_H_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Entropy (Continuous Random Variables)\n",
    "- Let X be a continuous random variable with cumulative distribution function F(x) = Pr(X<=x) and probability density function f(x).\n",
    "- The differential entropy h(X) of a continuous random variable X with density f(x) is defined as \n",
    "$$h(X) = -\\int_S f(x) log f(x)dx$$\n",
    "where $S$ is the support set of the random variable\n",
    "\n",
    "Example\n",
    "(Uniform distribution) Consider a continuous random variable distributed uniformly from 0 to $a$ so that its density i.e. $f(x)$ is $1/a$ from 0 to $a$ and 0 elsewhere. Then its differential entropy is\n",
    "$$h(X) = -\\int_0^a <mi>1</mi><mfract> log 1/a dx$$\n",
    "\n",
    "- Differential entropy can be negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
